# diary_of_science_work
Дневник для научной работы.

# diary_of_science_work
Дневник для научной работы.

## Chapter 1. Hyperparameter Optimization

В этой главе рассмотрены основные алгоритмы поиска гиперпараметров.

Model-free:  
>* grid search:
    Для каждого гиперпараметра выбирается множество значений,а затем оценивается каждый 
    элемент(определенный набор гиперпараметров) прямого произведения этих множеств.
* random grid search:
    Выбираются произвольные наборы гиперпараметров, способ хорош когда несколько гиперпараметров
    гораздо важнее остальных   
        
Population-based:
>* CMA-ES(covariance matrix adaption evolutionary strategy ):
    Гененрирует популяцию наборов гиперпараметров из многомерной Гауссианы, где параметры распределения
    обновляются от поколения к поколению основываясь на успешных представителях предыдущей популяции. 


Bayesian Optimization:
>* Основная идея состоит в следующих ключевых компонентах: вероятностная 'суррогатная' модель (предсказывает 
  фунцию потерь для данного набора гиперпараметров) и функционал для выбора нового набора гиперпараметров 
  для реальной оценки функции потерь и уточнения параметров модели-суррогата. Чаще всего в качестве такого
  функционала берут expected improvement. В качестве модели суррогата можно брать гауссовские процессы, 
  random forest и нейросети.

Multi-Fidelity Optimization:
>* Полное обучение модели на большом датасете может занимать до нескольких часов, это значит что оценка 
  даже одного набора гиперпараметров обходится непозволительно дорого. Предлагается использовать упрощенное 
  представление о функции потерь(low-fidelity), обучая модель на подмножестве всех объектов и признаков, с
  ограничением на число итераций.
* Один из способов уменьшения временных затрат является преждевременная остановка алгоритма по динамике
  кривой обучения(learning curve).
* Successive halving - на каждом шаге алгоритма отбрасывается худшая половина исследуемых наборов 
  гиперпараметров, ресурсы(выборка, время, вычислительные ресурсы) удваиваются и  делается аналогичный шаг.

Так же оговаривается что проблема автоматического обучения часто рассматривается шире, включая в себя автоматический отбор признаков, предобработку данных и выбор алгоритма.

## Chapter 2. Meta-Learning

