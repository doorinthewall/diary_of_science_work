{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kek:\n",
    "    * kek:\n",
    "        * lol\n",
    "        * superlol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# diary_of_science_work\n",
    "Дневник для научной работы.\n",
    "\n",
    "## Chapter 1. Hyperparameter Optimization\n",
    "\n",
    "В этой главе рассмотрены основные алгоритмы поиска гиперпараметров.\n",
    "\n",
    "Model-free:  \n",
    ">* grid search:\n",
    "    Для каждого гиперпараметра выбирается множество значений,а затем оценивается каждый \n",
    "    элемент(определенный набор гиперпараметров) прямого произведения этих множеств.\n",
    "* random grid search:\n",
    "    Выбираются произвольные наборы гиперпараметров, способ хорош когда несколько гиперпараметров\n",
    "    гораздо важнее остальных   \n",
    "        \n",
    "Population-based:\n",
    ">* CMA-ES(covariance matrix adaption evolutionary strategy ):\n",
    "    Гененрирует популяцию наборов гиперпараметров из многомерной Гауссианы, где параметры распределения\n",
    "    обновляются от поколения к поколению основываясь на успешных представителях предыдущей популяции. \n",
    "\n",
    "\n",
    "Bayesian Optimization:\n",
    ">* Основная идея состоит в следующих ключевых компонентах: вероятностная 'суррогатная' модель (предсказывает \n",
    "  фунцию потерь для данного набора гиперпараметров) и функционал для выбора нового набора гиперпараметров \n",
    "  для реальной оценки функции потерь и уточнения параметров модели-суррогата. Чаще всего в качестве такого\n",
    "  функционала берут expected improvement. В качестве модели суррогата можно брать гауссовские процессы, \n",
    "  random forest и нейросети.\n",
    "\n",
    "Multi-Fidelity Optimization:\n",
    ">* Полное обучение модели на большом датасете может занимать до нескольких часов, это значит что оценка \n",
    "  даже одного набора гиперпараметров обходится непозволительно дорого. Предлагается использовать упрощенное \n",
    "  представление о функции потерь(low-fidelity), обучая модель на подмножестве всех объектов и признаков, с\n",
    "  ограничением на число итераций.\n",
    "* Один из способов уменьшения временных затрат является преждевременная остановка алгоритма по динамике\n",
    "  кривой обучения(learning curve).\n",
    "* Successive halving - на каждом шаге алгоритма отбрасывается худшая половина исследуемых наборов \n",
    "  гиперпараметров, ресурсы(выборка, время, вычислительные ресурсы) удваиваются и  делается аналогичный шаг.\n",
    "\n",
    "Так же оговаривается что проблема автоматического обучения часто рассматривается шире, включая в себя автоматический отбор признаков, предобработку данных и выбор алгоритма.\n",
    "\n",
    "## Chapter 2. Meta-Learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kek:  \n",
    ">* Bullet 1\n",
    "* Bullet 2\n",
    "  * Bullet 2a\n",
    "  * Bullet 2b\n",
    "* Bullet 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
